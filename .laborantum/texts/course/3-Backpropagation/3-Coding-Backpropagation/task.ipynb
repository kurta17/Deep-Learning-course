{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json_tricks\n",
                "import numpy\n",
                "import random\n",
                "\n",
                "numpy.random.seed(0)\n",
                "random.seed(0)\n",
                "\n",
                "\n",
                "inputs1 = json_tricks.load(open('inputs1.json'))\n",
                "inputs2 = json_tricks.load(open('inputs2.json'))\n",
                "\n",
                "answer = {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "numpy.random.seed(0)\n",
                "random.seed(0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Implementing Backpropagation package for Numpy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this project, we will implement the backpropagation algorithm for numpy package.\n",
                "Your task will be to implement all the methods of the Node class that will be a wrapper for numpy arrays\n",
                "supporting backpropagation.\n",
                "\n",
                "Step-by-step, we will implement the following methods:\n",
                "\n",
                "1. `__init__` (a constructor)\n",
                "3. `backward` (a recursive mechanism that triggers backpropagation)\n",
                "4. `__neg__` (negation operator $- x$)\n",
                "5. `__add__` (addition operator $x + y$)\n",
                "6. `__mul__` (product operator $x \\cdot y$)\n",
                "7. `__sub__` (substitution operator $x - y$)\n",
                "8. `__truediv__` (division operator $x / y$)\n",
                "9. `exp` (exponentiation $\\exp(x)$)\n",
                "10. `sum` (summation $\\sum_{k} x_k$)\n",
                "11. `matmul` (matrix product $XY$)\n",
                "\n",
                "After that we will use the implemented methods on:\n",
                "1. Simple graph from the previous task\n",
                "2. Two-layer neural network\n",
                "\n",
                "Let's start!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`Node([1]) + Node([2]) -> Node`\n",
                "`(x + y).backward()`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 1\n",
                "\n",
                "Implement the `__init__` method for the `Node` class:\n",
                "- create field `data` and assign it to the `data` argument (we will need it to compute the gradient)\n",
                "- create field `grad` and assign it to the `None` (it will be used to store the gradient of the node)\n",
                "- create field `inputs` and assign it to the empty list (it will be used to store the nodes on which the current node depends)\n",
                "- create field `n_dependents` and assign it to the 0 (it will be used to count the number of nodes that depend on the current node)\n",
                "- create field `n_grads` and assign it to the 0 (it will be used to count the number of gradients that are coming to the current node)\n",
                "\n",
                "Implement the method `update_grad` for the `Node` class. The point of this method is to take care of the gradients that are coming from the next nodes and accumulate them:\n",
                "- firstly, create a deep copy of `grad_output` (just in case): `grad_output = copy.deepcopy(grad_output)`\n",
                "- increase the `n_grads` counter by 1 as one of the gradients of the dependents is registered\n",
                "- if `grad_output` is `None` and the size of `data` is 1, assign `grad_output` to the numpy array `[1]` (as we can calculate derivatives only of scalar values without any dependencies)\n",
                "- if `grad_output` is `None`, raise the `BaseException` with the message \"Backpropagation is impossible\" (as we can't compute the gradient without the gradient from the next node except for the case of a scalar value)\n",
                "- if `self.grad` is `None`, assign `grad_output` to the `self.grad` (that means that if there was no gradient before, we will just assign the one that we got from the next node)\n",
                "- otherwise, add `grad_output` to the `self.grad` (that means that if there was a gradient before, we will add the one that we got from the next node to the accumulated value)\n",
                "\n",
                "Implement the `backward` method for the `Node` class. This method is the heart of the backpropagation algorithm. And it logic is the following:\n",
                "- it waits and collects gradients from all the dependents of the current node\n",
                "- then it calculates the list of gradients for the inputs of the current node using the `backward_step` method (this method we will implement for every operation by hand)\n",
                "- triggers the `backward` method of the inputs with the calculated gradient in the current node as an argument\n",
                "\n",
                "As a formula, it looks like this:\n",
                "$\\partial_w {L} = \\sum_{k=1}^{n} \\partial_w {z_k} \\partial_{z_k} {L}$\n",
                "\n",
                "So, how this works in practice?\n",
                "- call the `update_grad` method with the `grad_output` argument\n",
                "- if we have collected all the gradients from the dependents, it is time to calculate the gradients for the inputs of the current node \n",
                "- call the `backward_step` method with the `grad_output` argument to calculate the gradients for the inputs\n",
                "- call the `backward` method of the inputs with the calculated gradient in the current node as an argument\n",
                "- reset the `n_grads` counter to 0 (actually, this is not necessary as we do backpropagation only once)\n",
                "\n",
                "Also `backward_step` method should be implemented for basic node too. It should return an empty list as basic node does not have any inputs.\n",
                "\n",
                "In general, we will sotre inputs to the operation as list of nodes in the `inputs` field. The `backward_step` function should return the corresponding list of gradients for the inputs.\n",
                "\n",
                "After these steps are done, you can already check that the first test passes successfully\n",
                "\n",
                "# Task 2\n",
                "\n",
                "Implement the `__add__` method for the `Node` class and the class `TensorSum` that inherits from `Node`. \n",
                "This method enables to use the `+` operator for a pair of objects of `Node` class and means that after we perofrm `z = x + y`, result\n",
                "of this operation is also a Node that correspond to the sum of two nodes.\n",
                "\n",
                "The engine for that operation is `TensorSum` class, here you need to:\n",
                "- initialize the `Node` class with the `None` argument (empty envelope of the node) in the `__init__` method\n",
                "- in the `__call__` method (this method is used when we perform `z = x + y`):\n",
                "    - assign the `inputs` fields of the current node to the `[input1, input2]` to store the inputs\n",
                "    - set `n_dependents` to the sum of `n_dependents` of the inputs (or simply to 2 as we have only 2 inputs)\n",
                "    - calculate value of `data` field of the current node using the `self.inputs[0].data + self.inputs[1].data` formula\n",
                "    - set the `grad` field of the current node to `None` as we don't have any gradient yet\n",
                "    - return the `self` node (so that the result of the operation is the current node)\n",
                "- in the `backward_step` method:\n",
                "    - return the list of gradients for the inputs: `[grad_output, grad_output]` as $\\partial_x {x + y} = 1$ and $\\partial_y {x + y} = 1$\n",
                "\n",
                "\n",
                "In `Node` class:\n",
                "- implement the `__add__` method:\n",
                "    - create a new `TensorSum` object with the sum of the `data` of the current node and the `other` node and return it (`return TensorSum()(self, other)`)\n",
                "\n",
                "Now your Node class supports the `+` operator. You can check it by runnint test number 2. So you can calculate gradients of expressions like `x + y + x + x + y + y`. That is no much, but that is a good start.\n",
                "\n",
                "# Task 3 and others\n",
                "\n",
                "Implement the remaining operations by analogy:\n",
                "- `__sub__` that will be used when we perform `z = x - y`\n",
                "- `__mul__` that will be used when we perform `z = x * y`\n",
                "- `__truediv__` that will be used when we perform `z = x / y`\n",
                "- `__neg__` that will be used when we perform `z = -x`\n",
                "- `exp` that will be used when we perform `z = numpy.exp(x)`\n",
                "- `sum` that will be used when we perform `z = numpy.sum(x)`\n",
                "- `matmul` that will be used when we perform `z = x @ y`\n",
                "\n",
                "In all these cases, you should use theoretircal derivatives to implement `backward_step` function for every of these operations.\n",
                "\n",
                "The hardest one will be `matmul` as it is a matrix multiplication. To implement it, use the following rule of thumb:\n",
                "- the resulting gradient should have the same shape as data\n",
                "- the gradients of linear functions are also some linear functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import copy\n",
                "\n",
                "# Node(np.array([1]))\n",
                "class Node:\n",
                "    def __init__(self, data):\n",
                "        if not isinstance(data, np.ndarray):\n",
                "            self.data = np.array(data)\n",
                "        else:\n",
                "            self.data = data\n",
                "        self.grad = None\n",
                "        self.inputs = []\n",
                "        self.n_dependents = 0 \n",
                "        self.n_grads = 0\n",
                "\n",
                "    def update_grad(self, grad_output):\n",
                "        grad_output = copy.deepcopy(grad_output)\n",
                "        self.n_grads += 1\n",
                "        if grad_output is None:\n",
                "            if self.data.size == 1:\n",
                "                grad_output = np.array([1], dtype=self.data.dtype)\n",
                "            else:\n",
                "                raise BaseException(\"Backpropagation is impossible\")\n",
                "\n",
                "        if self.grad is None:\n",
                "            self.grad = grad_output\n",
                "        else:\n",
                "            self.grad += grad_output\n",
                "\n",
                "\n",
                "    def backward(self, grad_output=None):\n",
                "        self.update_grad(grad_output)\n",
                "        # Make sure the check for complete gradients is working properly\n",
                "        if self.n_grads == self.n_dependents or self.n_dependents == 0:\n",
                "            input_grads = self.backward_step(self.grad)\n",
                "            for i, inp in enumerate(self.inputs):\n",
                "                if i < len(input_grads):\n",
                "                    inp.backward(input_grads[i])\n",
                "            # Reset gradient count after backward pass\n",
                "            self.n_grads = 0\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        return []\n",
                "\n",
                "    # @backprop\n",
                "    # def backward(self, grad_output=None):\n",
                "\n",
                "\n",
                "    def __str__(self):\n",
                "        return f\"Node(data={self.data}, grad={self.grad})\"\n",
                "\n",
                "    def __neg__(self):\n",
                "        return TensorNeg()(self)\n",
                "\n",
                "    def __add__(self, other):\n",
                "        return TensorSum()(self, other)\n",
                "\n",
                "    def __mul__(self, other):\n",
                "        return TensorMul()(self, other)\n",
                "\n",
                "    def __sub__(self, other):\n",
                "        return TensorSub()(self, other)\n",
                "\n",
                "    def __truediv__(self, other):\n",
                "        return TensorDiv()(self, other)\n",
                "\n",
                "    def exp(self):\n",
                "        return TensorExp()(self)\n",
                "\n",
                "    def sum(self, axis=None):\n",
                "        return TensorSumReduce()(self)\n",
                "\n",
                "    def __matmul__(self, other):\n",
                "        return TensorMatMul()(self, other)\n",
                "\n",
                "\n",
                "class TensorSum(Node):\n",
                "    def __init__(self):\n",
                "        super().__init__(None)\n",
                "\n",
                "    def __call__(self, input1, input2):\n",
                "        self.inputs = [input1, input2]\n",
                "\n",
                "        input1.n_dependents += 1\n",
                "        input2.n_dependents += 1\n",
                "        self.data = input1.data + input2.data\n",
                "        self.grad = None\n",
                "        return self\n",
                "    \n",
                "    def backward_step(self, grad_output=None):\n",
                "        return [grad_output, grad_output] \n",
                "\n",
                "\n",
                "class TensorSub(Node):\n",
                "    def __init__(self):\n",
                "        super().__init__(None)\n",
                "\n",
                "    def __call__(self, input1, input2):\n",
                "        self.inputs = [input1, input2]\n",
                "        input1.n_dependents += 1\n",
                "        input2.n_dependents += 1\n",
                "        self.data = self.inputs[0].data - self.inputs[1].data\n",
                "        self.grad = None\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        # d(x-y)/dx = 1, d(x-y)/dy = -1\n",
                "        return [grad_output, -grad_output]\n",
                "\n",
                "class TensorMul(Node):\n",
                "    def __init__(self):\n",
                "        super().__init__(None)\n",
                "\n",
                "    def __call__(self, input1, input2):\n",
                "        self.inputs = [input1, input2]\n",
                "        input1.n_dependents += 1\n",
                "        input2.n_dependents += 1\n",
                "        self.data = self.inputs[0].data * self.inputs[1].data\n",
                "        self.grad = None\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        # d(x*y)/dx = y, d(x*y)/dy = x\n",
                "        return [grad_output * self.inputs[1].data, grad_output * self.inputs[0].data]\n",
                "\n",
                "\n",
                "\n",
                "class TensorDiv(Node):\n",
                "    def __init__(self):\n",
                "        super().__init__(None)\n",
                "\n",
                "    def __call__(self, input1, input2):\n",
                "        self.inputs = [input1, input2]\n",
                "        input1.n_dependents += 1\n",
                "        input2.n_dependents += 1\n",
                "        self.data = self.inputs[0].data / self.inputs[1].data\n",
                "        self.grad = None\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        # d(x/y)/dx = 1/y, d(x/y)/dy = -x/y^2\n",
                "        x = self.inputs[0].data\n",
                "        y = self.inputs[1].data\n",
                "        return [grad_output / y, -grad_output * x / (y * y)]\n",
                "\n",
                "class TensorNeg(Node):\n",
                "    def __init__(self):\n",
                "        super().__init__(None)\n",
                "\n",
                "    def __call__(self, input1):\n",
                "        self.inputs = [input1]\n",
                "        input1.n_dependents += 1\n",
                "        self.data = -self.inputs[0].data\n",
                "        self.grad = None\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        # d(-x)/dx = -1\n",
                "        return [-grad_output]\n",
                "\n",
                "\n",
                "class TensorExp(Node):\n",
                "    def __init__(self):\n",
                "        super().__init__(None)\n",
                "\n",
                "    def __call__(self, input1):\n",
                "        self.inputs = [input1]\n",
                "        input1.n_dependents += 1\n",
                "        self.data = np.exp(self.inputs[0].data)\n",
                "        self.grad = None\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        # d(exp(x))/dx = exp(x)\n",
                "        return [grad_output * self.data]\n",
                "\n",
                "\n",
                "class TensorSumReduce(Node):\n",
                "    def __init__(self):\n",
                "        super().__init__(None)\n",
                "\n",
                "    def __call__(self, input1):\n",
                "        self.inputs = [input1]\n",
                "        input1.n_dependents += 1\n",
                "        self.data = np.sum(self.inputs[0].data)\n",
                "        self.grad = None\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        # d(sum(x))/dx = ones_like(x)\n",
                "        return [np.ones_like(self.inputs[0].data) * grad_output]\n",
                "    \n",
                "class TensorMatMul(Node):\n",
                "    def __init__(self):\n",
                "        super().__init__(None)\n",
                "        \n",
                "    def __call__(self, input1, input2):\n",
                "        self.inputs = [input1, input2]\n",
                "        input1.n_dependents += 1\n",
                "        input2.n_dependents += 1\n",
                "        self.data = input1.data @ input2.data  # Matrix multiplication\n",
                "        self.grad = None\n",
                "        return self\n",
                "        \n",
                "    def backward_step(self, grad_output=None):\n",
                "        # C = A @ B:\n",
                "        # ∂L/∂A = ∂L/∂C @ B.T\n",
                "        # ∂L/∂B = A.T @ ∂L/∂C\n",
                "        return [\n",
                "            grad_output @ self.inputs[1].data.T, \n",
                "            self.inputs[0].data.T @ grad_output\n",
                "        ]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Node(data=[1], grad=None)\n",
                        "Node(data=[1], grad=[1])\n"
                    ]
                }
            ],
            "source": [
                "# TEST 1\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "print(x)\n",
                "x.backward()\n",
                "print(x)\n",
                "\n",
                "answer['init'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "\n",
                "    x.backward()\n",
                "\n",
                "    answer['init'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Node(data=[8], grad=None)\n",
                        "Node(data=[1], grad=[4]) Node(data=[2], grad=[2])\n"
                    ]
                }
            ],
            "source": [
                "# TEST 2\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "z = x + y + x + x + x + y\n",
                "\n",
                "print(z)\n",
                "z.backward()\n",
                "print(x, y)\n",
                "\n",
                "answer['sum'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "    y = Node(inp['x'][1])\n",
                "\n",
                "    z = x + y + x + x + x + y\n",
                "    z.backward()\n",
                "\n",
                "    answer['sum'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Node(data=[-2], grad=None)\n",
                        "Node(data=[1], grad=[2]) Node(data=[2], grad=[-2])\n"
                    ]
                }
            ],
            "source": [
                "# TEST 3\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "z = x - y + x + x - x - y\n",
                "\n",
                "print(z)\n",
                "z.backward()\n",
                "print(x, y)\n",
                "\n",
                "answer['diff'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "    y = Node(inp['x'][1])\n",
                "\n",
                "    z = x - y + x + x - y - y\n",
                "    z.backward()\n",
                "\n",
                "    answer['diff'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Node(data=[-3], grad=None)\n",
                        "Node(data=[1], grad=[2]) Node(data=[2], grad=[-4])\n"
                    ]
                }
            ],
            "source": [
                "# TEST 4\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "z = (x + y) * (x - y)\n",
                "\n",
                "print(z)\n",
                "z.backward()\n",
                "print(x, y)\n",
                "\n",
                "answer['mul'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "    y = Node(inp['x'][1])\n",
                "\n",
                "    z = (x + y) * (x - y) * (x + x + y)\n",
                "    z.backward()\n",
                "\n",
                "    answer['mul'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Node(data=[0.5], grad=None)\n",
                        "Node(data=[1], grad=[0.5]) Node(data=[2], grad=[-0.25])\n"
                    ]
                }
            ],
            "source": [
                "# TEST 5\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "z = x / y\n",
                "\n",
                "print(z)\n",
                "z.backward()\n",
                "print(x, y)\n",
                "\n",
                "answer['div'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "    y = Node(inp['x'][1])\n",
                "\n",
                "    z = x / (Node(0.5) + y)\n",
                "    z.backward()\n",
                "\n",
                "    answer['div'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Node(data=[-1], grad=None)\n",
                        "Node(data=[1], grad=[-1])\n"
                    ]
                }
            ],
            "source": [
                "# TEST 6\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "z = -x\n",
                "\n",
                "print(z)\n",
                "z.backward()\n",
                "print(x)\n",
                "\n",
                "answer['neg'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "\n",
                "    z = -x\n",
                "    z.backward()\n",
                "\n",
                "    answer['neg'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Node(data=[20.08553692], grad=None)\n",
                        "None None\n",
                        "[20.08553692] [20.08553692]\n"
                    ]
                }
            ],
            "source": [
                "# TEST 7\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "z = (x + y).exp()\n",
                "\n",
                "print(z)\n",
                "print(x.grad, y.grad)\n",
                "\n",
                "z.backward()\n",
                "\n",
                "print(x.grad, y.grad)\n",
                "\n",
                "answer['exp'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "\n",
                "    z = x.exp()\n",
                "    z.backward()\n",
                "\n",
                "    answer['exp'].append(x.grad)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task\n",
                "\n",
                "Implement Graph function from the previous task. For the constants please use `Node(1)` whenever needed (otherwise you will get an error)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "OUR RESULTS:\n",
                        "[-9.177064e-07] [-1.8354128e-06] [-8.47644006e-38] [7.26552005e-38] [2.14131493e-06] [-1.8354128e-06]\n",
                        "TORCH RESULTS:\n",
                        "tensor([-9.1771e-07, -1.8354e-06], dtype=torch.float64) tensor([ 0.0000e+00,  0.0000e+00,  2.1413e-06, -1.8354e-06],\n",
                        "       dtype=torch.float64)\n",
                        "TORCH RESULTS:\n",
                        "tensor([-9.1771e-07, -1.8354e-06], dtype=torch.float64) tensor([ 0.0000e+00,  0.0000e+00,  2.1413e-06, -1.8354e-06],\n",
                        "       dtype=torch.float64)\n"
                    ]
                }
            ],
            "source": [
                "# TEST 8 (Graph)\n",
                "\n",
                "def sigmoid(z):\n",
                "    # Use explicit floating point constants throughout to ensure numerical stability\n",
                "    return Node(1.0) / (Node(1.0) + (-z).exp())\n",
                "\n",
                "def tanh(z):\n",
                "    # Ensure we're using stable computation for tanh\n",
                "    # Avoid potential numerical issues by using floating point constants\n",
                "    exp_z = z.exp()\n",
                "    exp_neg_z = (-z).exp()\n",
                "    return (exp_z - exp_neg_z) / (exp_z + exp_neg_z)\n",
                "\n",
                "def graph_value(x, w):\n",
                "    t = tanh(w[0] * x[0] + w[1] * x[1])\n",
                "    s = sigmoid(w[2] * x[0] + w[3] * x[1])\n",
                "    y = t * s\n",
                "    return y\n",
                "\n",
                "answer['graph'] = []\n",
                "for inp in inputs1:\n",
                "\n",
                "    x = inp['x']\n",
                "    w = inp['w']\n",
                "\n",
                "    x = [Node(float(val)) for val in x]\n",
                "    w = [Node(float(val)) for val in w]\n",
                "\n",
                "    y = graph_value(x, w)\n",
                "    y.backward()\n",
                "\n",
                "    answer['graph'].append([x[0].grad, x[1].grad, w[0].grad, w[1].grad, w[2].grad, w[3].grad])\n",
                "\n",
                "print(\"OUR RESULTS:\")\n",
                "print(x[0].grad, x[1].grad, w[0].grad, w[1].grad, w[2].grad, w[3].grad)\n",
                "\n",
                "def torch_graph_value(x, w):\n",
                "    t = torch.tanh(w[0] * x[0] + w[1] * x[1])\n",
                "    s = torch.sigmoid(w[2] * x[0] + w[3] * x[1])\n",
                "    y = t * s\n",
                "    return y\n",
                "\n",
                "import torch\n",
                "x = torch.tensor(inp['x'], requires_grad=True, dtype=float)\n",
                "w = torch.tensor(inp['w'], requires_grad=True, dtype=float)\n",
                "\n",
                "y = torch_graph_value(x, w)\n",
                "y.backward()\n",
                "\n",
                "print(\"TORCH RESULTS:\")\n",
                "print(x.grad, w.grad)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2-layer NN\n",
                "\n",
                "Implement 2 layer Neural Network and compute its gradient using `Node` class:\n",
                "\n",
                "$\\mathbf y = \\sigma( W_2 \\sigma(W_1 \\mathbf x + \\mathbf b_1) + \\mathbf b_2)$\n",
                "\n",
                "Return sum of all values in $y * y$ as loss function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 9 (Two-layer net)\n",
                "\n",
                "def two_layer_net(x, W1, W2, b1, b2):\n",
                "    def sigmoid(z):\n",
                "        return Node(1.0) / (Node(1.0) + (-z).exp())\n",
                "    \n",
                "    x_t = x if x.data.shape[0] == 1 else Node(x.data.T)\n",
                "    W1_t = Node(W1.data.T)\n",
                "    W2_t = Node(W2.data.T)\n",
                "    b1_t = Node(b1.data.T) if b1.data.shape[0] != 1 else b1\n",
                "    b2_t = Node(b2.data.T) if b2.data.shape[0] != 1 else b2\n",
                "    \n",
                "    h1 = sigmoid(x_t @ W1_t + b1_t)\n",
                "    y = sigmoid(h1 @ W2_t + b2_t)\n",
                "    loss = (y * y).sum()\n",
                "    \n",
                "    return loss\n",
                "\n",
                "\n",
                "answer['two_layer_net'] = []\n",
                "for inp in inputs2:\n",
                "    x = Node(inp['x'])\n",
                "    W1 = Node(inp['W1'])\n",
                "    W2 = Node(inp['W2']) \n",
                "    b1 = Node(inp['b1'])\n",
                "    b2 = Node(inp['b2'])\n",
                "    \n",
                "    h_hat = two_layer_net(x, W1, W2, b1, b2)\n",
                "    h_hat.backward()\n",
                "    \n",
                "    answer['two_layer_net'].append([x.grad, W1.grad, W2.grad, b1.grad, b2.grad])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conclusion\n",
                "\n",
                "You have implemented a backpropagation algorithm. This algorithm is similar to one that is used in Torch. Note that you have implemented all the mechanics of it. Thus it should be now not a magical box: you know exactly how it works."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_tricks.dump(answer, '.answer.json')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (Container)",
            "language": "python",
            "name": "container_env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
